{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9#:~:text=HyperParameter%20Tuning%20%E2%80%94%20Hyperopt%20Bayesian%20Optimization%20for%20(Xgboost%20and%20Neural%20network),-Hyperparameters%3A%20These%20are&text=HYPEROPT%3A%20It%20is%20a%20powerful,TPE%20(Tree%20Parzen%20Estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------ \n",
    "# import packages\n",
    "#------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import date\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classification imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "#from hyperopt import fmin, hp, tpe, rand, Trials, STATUS_OK\n",
    "import pickle\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_traintestsplit mod is imported into another module\n",
      "train_predict_model_xgb_tpe mod is imported into another module\n",
      "visualize mod is imported into another module\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import models\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../data/raw/'\n",
    "df_labels = pd.read_csv(path_to_file + 'major_donor_labels.csv')\n",
    "df_donations = pd.read_csv(path_to_file + 'donations.csv')\n",
    "df_features = pd.read_csv(path_to_file + 'windfall_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130114, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean df_labels\n",
    "# rename and drop unnecessary columns\n",
    "df_labels = df_labels.drop('Unnamed: 0', axis=1)\n",
    "df_labels.set_axis(['candidate_id', 'ideal_donor'], axis=1, inplace=True)\n",
    "df_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donations.dropna(how='any', inplace=True)\n",
    "df_donations.reset_index(drop=True, inplace=True)\n",
    "df_donations.set_axis(['candidate_id', 'trans_date', 'amount'], axis=1, inplace=True)\n",
    "df_donations['trans_date'] = pd.to_datetime(df_donations['trans_date']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2016-08-03\n",
       "max   2021-07-30\n",
       "Name: trans_date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get original amount column for candidate_id's. Will use this to measure correlations of features to target.\n",
    "\n",
    "# get \"ideal donors\" as specified by prompt \n",
    "df_donations_idealdonors = df_donations.loc[~(df_donations['amount'] < 0) & ((df_donations['trans_date'] < '2021-07-31') & (df_donations['trans_date'] > '2016-08-01')), ['candidate_id','trans_date','amount']]\n",
    "df_donations_idealdonors['trans_date'].agg(['min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donations_idealdonors_aggamount = df_donations_idealdonors.groupby('candidate_id')['amount'].sum().reset_index()\n",
    "del df_donations_idealdonors\n",
    "\n",
    "# create target label\n",
    "fx = lambda x: (1 if x >= 20000 else 0)\n",
    "df_donations_idealdonors_aggamount['target'] = df_donations_idealdonors_aggamount['amount'].apply(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check = 0: 0\n"
     ]
    }
   ],
   "source": [
    "df_labels_idealdonors = df_labels.join(df_donations_idealdonors_aggamount.set_index('candidate_id'), on=['candidate_id'], how='left').fillna(0)\n",
    "print('check = 0: %d' %(df_labels_idealdonors['ideal_donor'] != df_labels_idealdonors['target']).sum())\n",
    "#df_labels_idealdonors.drop(columns=['ideal_donor','target'], inplace=True)\n",
    "\n",
    "df_labels_idealdonors.drop(columns=['ideal_donor','amount'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['totalHouseholdDebt',\n",
       " 'primaryPropertyLoanToValue',\n",
       " 'primaryPropertyValue',\n",
       " 'propertyCount',\n",
       " 'NetWorth']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# independent variables\n",
    "ind_features = [name for name in df_features.columns if name.find('Class') == -1 and name.find('Cause') == -1]\n",
    "ind_features = ind_features[1:]\n",
    "ind_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and add feature engineered columns to datasets\n",
    "df_features_addfeatures = preprocess.create_features_df_features(df_features, ind_features, ID='candidate_id')\n",
    "df_donations_addfeatures = preprocess.create_features_df_donations(df_donations, PredictedOn='2016-08-01')\n",
    "\n",
    "# join datasets\n",
    "df_donorfeatures = df_features_addfeatures.join(df_donations_addfeatures.set_index('candidate_id'), on=['candidate_id'], how='inner')\n",
    "\n",
    "# add col of random int to check baseline feature importance\n",
    "df_donorfeatures['random_value'] = np.random.randint(0,100, size=len(df_donorfeatures))\n",
    "\n",
    "# add scaled amount_prev by NetWorth\n",
    "df_donorfeatures['amountscaled_prev360d3'] = df_donorfeatures['amount_prev360d3'] / df_donorfeatures['NetWorth']\n",
    "df_donorfeatures['amountscaled_prev360d5'] = df_donorfeatures['amount_prev360d5'] / df_donorfeatures['NetWorth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dependent variable\n",
    "df_final = df_donorfeatures.join(df_labels_idealdonors.set_index('candidate_id'), on=['candidate_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>primaryPropertyValue</th>\n",
       "      <th>propertyCount</th>\n",
       "      <th>NetWorth</th>\n",
       "      <th>primaryPropertyLoanToValue_ideal</th>\n",
       "      <th>primaryPropertyValueToNetWorth_ratio</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>amount_prev360d2</th>\n",
       "      <th>amount_prev360d3</th>\n",
       "      <th>amount_prev360d4</th>\n",
       "      <th>amount_prev360d5</th>\n",
       "      <th>count_trans_date_prev5y</th>\n",
       "      <th>random_value</th>\n",
       "      <th>amountscaled_prev360d3</th>\n",
       "      <th>amountscaled_prev360d5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_0</td>\n",
       "      <td>2215000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14011369.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158086</td>\n",
       "      <td>7.450820e+05</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1800.01</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_1</td>\n",
       "      <td>3650000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5812754.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627930</td>\n",
       "      <td>3.024625e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_2</td>\n",
       "      <td>625000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1060001.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.589622</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_3</td>\n",
       "      <td>903455.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4237949.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213182</td>\n",
       "      <td>2.680700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate_4</td>\n",
       "      <td>2608000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10013587.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260446</td>\n",
       "      <td>1.110278e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  candidate_id  primaryPropertyValue  propertyCount    NetWorth  \\\n",
       "0  candidate_0             2215000.0            4.0  14011369.0   \n",
       "1  candidate_1             3650000.0            1.0   5812754.0   \n",
       "2  candidate_2              625000.0            1.0   1060001.0   \n",
       "3  candidate_3              903455.0            3.0   4237949.0   \n",
       "4  candidate_4             2608000.0            1.0  10013587.0   \n",
       "\n",
       "   primaryPropertyLoanToValue_ideal  primaryPropertyValueToNetWorth_ratio  \\\n",
       "0                                 1                              0.158086   \n",
       "1                                 0                              0.627930   \n",
       "2                                 1                              0.589622   \n",
       "3                                 1                              0.213182   \n",
       "4                                 1                              0.260446   \n",
       "\n",
       "     LoanAmount  amount_prev360d2  amount_prev360d3  amount_prev360d4  \\\n",
       "0  7.450820e+05            2100.0           1800.01            4500.0   \n",
       "1  3.024625e+06               0.0              0.00               0.0   \n",
       "2  1.000000e+00               0.0              0.00               0.0   \n",
       "3  2.680700e+04               0.0              0.00               0.0   \n",
       "4  1.110278e+06               0.0              0.00               0.0   \n",
       "\n",
       "   amount_prev360d5  count_trans_date_prev5y  random_value  \\\n",
       "0            1100.0                   2122.0            30   \n",
       "1               0.0                      0.0             8   \n",
       "2               0.0                      0.0            12   \n",
       "3               0.0                      0.0            47   \n",
       "4               0.0                      0.0            50   \n",
       "\n",
       "   amountscaled_prev360d3  amountscaled_prev360d5  target  \n",
       "0                0.000128                0.000079     1.0  \n",
       "1                0.000000                0.000000     1.0  \n",
       "2                0.000000                0.000000     0.0  \n",
       "3                0.000000                0.000000     0.0  \n",
       "4                0.000000                0.000000     0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50610 entries, 0 to 50918\n",
      "Data columns (total 16 columns):\n",
      "candidate_id                            50610 non-null object\n",
      "primaryPropertyValue                    50610 non-null float64\n",
      "propertyCount                           50610 non-null float64\n",
      "NetWorth                                50610 non-null float64\n",
      "primaryPropertyLoanToValue_ideal        50610 non-null int64\n",
      "primaryPropertyValueToNetWorth_ratio    50610 non-null float64\n",
      "LoanAmount                              50610 non-null float64\n",
      "amount_prev360d2                        50610 non-null float64\n",
      "amount_prev360d3                        50610 non-null float64\n",
      "amount_prev360d4                        50610 non-null float64\n",
      "amount_prev360d5                        50610 non-null float64\n",
      "count_trans_date_prev5y                 50610 non-null float64\n",
      "random_value                            50610 non-null int64\n",
      "amountscaled_prev360d3                  50610 non-null float64\n",
      "amountscaled_prev360d5                  50610 non-null float64\n",
      "target                                  50610 non-null float64\n",
      "dtypes: float64(13), int64(2), object(1)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_split(df, cnames_to_drop=['ideal_donor', 'candidate_id'], target_col='ideal_donor'):\n",
    "    '''\n",
    "    Args:\n",
    "        df (DataFrame): data to be analyzed\n",
    "        cnames_to_drop (list): list of names to be dropped from df\n",
    "        target_col (str): name of column of true values. \n",
    "    Returns:\n",
    "        DataFrame of X values\n",
    "        Series of y values\n",
    "    ''' \n",
    "    # get feature names\n",
    "    fset=[x for x in df if x not in cnames_to_drop]\n",
    "    \n",
    "    # Breaking up preprocessed data into predictor and target\n",
    "    X=df[fset]\n",
    "    y=df[target_col]\n",
    "    \n",
    "    print('Metadata about full dataset:')\n",
    "    print('    number of members in full dataset: %d' % len(X))\n",
    "    print('    number of features in full dataset = %d' % len(fset))\n",
    "    print('    number of classes in full dataset : %d \\n' %y.nunique())\n",
    "    print('')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata about full dataset:\n",
      "    number of members in full dataset: 50610\n",
      "    number of features in full dataset = 14\n",
      "    number of classes in full dataset : 2 \n",
      "\n",
      "\n",
      "count per class: 50225 385\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "# split data into train and test\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#target_col = 'amount'\n",
    "#cnames_to_drop = ['amount', 'candidate_id']\n",
    "\n",
    "target_col = 'target'\n",
    "cnames_to_drop = ['target', 'candidate_id']\n",
    "test_size = 0.30\n",
    "random_state = 42\n",
    "\n",
    "X, y = feature_target_split(df_final, cnames_to_drop=cnames_to_drop, target_col=target_col)\n",
    "\n",
    "print('count per class: %d %d' %(y.value_counts()[0], y.value_counts()[1]))\n",
    "#Xtrain, Xtest, ytrain, ytest = train_test_split(X.values, y.values, test_size=test_size, stratify=None, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ytrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30450c4574c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of target members in train: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of target members in test: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ytrain' is not defined"
     ]
    }
   ],
   "source": [
    "print('number of target members in train: %d' %(ytrain>20000).astype('float').sum())\n",
    "print('number of target members in test: %d' % (ytest>20000).astype('float').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import uniform #as sp_randFloat\n",
    "#from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute search\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0208            2.17m\n",
      "         2           0.0127            1.47m\n",
      "         3           0.0115            1.30m\n",
      "         4           0.0104            1.18m\n",
      "         5           0.0094            1.10m\n",
      "         6           0.0085            1.05m\n",
      "         7           0.0077           59.91s\n",
      "         8           0.0070           58.09s\n",
      "         9           0.0063           56.98s\n",
      "        10           0.0057           56.37s\n",
      "        20           0.0022           53.17s\n",
      "        30           0.0009           47.73s\n",
      "        40           0.0004           45.16s\n",
      "        50           0.0002           44.73s\n",
      ">outer_test=0.380, inner_test=0.628, cfg={'subsample': 1.0, 'max_features': 10, 'max_depth': 19.0}\n",
      "execute search\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0280            1.61m\n",
      "         2           0.0148            1.31m\n",
      "         3           0.0119            1.21m\n",
      "         4           0.0107            1.15m\n",
      "         5           0.0097            1.14m\n",
      "         6           0.0088            1.05m\n",
      "         7           0.0079            1.02m\n",
      "         8           0.0072            1.01m\n",
      "         9           0.0065           59.98s\n",
      "        10           0.0059            1.01m\n",
      "        20           0.0022           53.09s\n",
      "        30           0.0009           49.47s\n",
      "        40           0.0004           47.64s\n",
      "        50           0.0002           46.05s\n",
      ">outer_test=0.260, inner_test=0.611, cfg={'subsample': 1.0, 'max_features': 9, 'max_depth': 12.0}\n",
      "execute search\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0305           0.0099            1.07m\n",
      "         2           0.0182          -0.2147           57.65s\n",
      "         3           0.0366           0.0003            1.01m\n",
      "         4           0.0353           0.0010            1.05m\n",
      "         5           0.0342           0.0010            1.07m\n",
      "         6           0.0330           0.0009            1.08m\n",
      "         7           0.0320           0.0008            1.11m\n",
      "         8           0.0313           0.0007            1.10m\n",
      "         9           0.0306           0.0007            1.10m\n",
      "        10           0.0299           0.0006            1.09m\n",
      "        20           0.0260           0.0002           59.67s\n",
      "        30           0.0246           0.0001           54.61s\n",
      "        40           0.0240           0.0000           50.50s\n",
      "        50           0.0238           0.0000           44.48s\n",
      ">outer_test=0.350, inner_test=0.629, cfg={'subsample': 0.8999999999999999, 'max_features': 6, 'max_depth': 13.0}\n",
      "execute search\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0209            2.82m\n",
      "         2           0.0131            2.37m\n",
      "         3           0.0118            2.38m\n",
      "         4           0.0107            2.39m\n",
      "         5           0.0096            2.44m\n",
      "         6           0.0087            2.41m\n",
      "         7           0.0079            2.40m\n",
      "         8           0.0071            2.40m\n",
      "         9           0.0065            2.39m\n",
      "        10           0.0059            2.39m\n",
      "        20           0.0022            2.25m\n",
      "        30           0.0009            2.02m\n",
      "        40           0.0004            1.87m\n",
      "        50           0.0002            1.71m\n",
      ">outer_test=0.482, inner_test=0.555, cfg={'subsample': 1.0, 'max_features': 14, 'max_depth': 17.0}\n",
      "execute search\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0345            1.28m\n",
      "         2           0.0258            1.15m\n",
      "         3           0.0189            1.12m\n",
      "         4           0.0154            1.10m\n",
      "         5           0.0123            1.13m\n",
      "         6           0.0104            1.11m\n",
      "         7           0.0095            1.13m\n",
      "         8           0.0087            1.15m\n",
      "         9           0.0079            1.16m\n",
      "        10           0.0073            1.18m\n",
      "        20           0.0028            1.19m\n",
      "        30           0.0011            1.12m\n",
      "        40           0.0005            1.06m\n",
      "        50           0.0003           59.45s\n",
      ">outer_test=0.447, inner_test=0.591, cfg={'subsample': 1.0, 'max_features': 7, 'max_depth': 11.0}\n",
      "mean estimated performance: 0.384 (0.077)\n",
      "CPU times: user 34.2 s, sys: 231 ms, total: 34.5 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "random_state = 42\n",
    "learning_rate = .1\n",
    "n_estimators = 500\n",
    "n_iter_no_change = 50\n",
    "validation_fraction = 0.1 # fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model.\n",
    "tol = 0.02 # if the scores don't improve by at least 0.01 for the last X stages, stop fitting additional stages\n",
    "verbose = 1\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "best_inner_params_list = []\n",
    "for train_ix, test_ix in cv_outer.split(X.values, y.values):\n",
    "    # split data\n",
    "    X_train, X_test = X.values[train_ix, :], X.values[test_ix, :]\n",
    "    y_train, y_test = y.values[train_ix], y.values[test_ix]\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # define the model\n",
    "    model = GradientBoostingClassifier(random_state=random_state, learning_rate=learning_rate, \n",
    "                                       n_estimators=n_estimators, n_iter_no_change=n_iter_no_change, \n",
    "                                       tol=tol, validation_fraction=validation_fraction, verbose=verbose)\n",
    "    \n",
    "    # define search space\n",
    "    #params_fixed = {'learning_rate'    :[learning_rate],  \n",
    "    #                'n_estimators'     :[n_estimators],  \n",
    "    #                'random_state'     :[random_state],\n",
    "    #                'n_iter_no_change ':[n_iter_no_change],\n",
    "    #                'verbose'          :[1]\n",
    "    #               } \n",
    "        \n",
    "    param_space = {'max_depth'        : np.linspace(5, 20, 16, endpoint=True),\n",
    "                   'subsample'        : np.linspace(.4, 1, 7, endpoint=True),\n",
    "                   #'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                   #'min_samples_leaf' : np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                   'max_features'     : list(range(1,X.shape[1]+1))\n",
    "                  }\n",
    "    #param_space.update(params_fixed)\n",
    "    \n",
    "    # define search\n",
    "    scoring = 'average_precision' # 'neg_mean_squared_error'\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_space, scoring=scoring, n_jobs=-1, \n",
    "                                cv=cv_inner, refit=True, verbose=0)\n",
    "    \n",
    "    # execute search\n",
    "    print('execute search')\n",
    "    result = search.fit(X_train, y_train)\n",
    "    \n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    score = average_precision_score(y_test, yhat)\n",
    "    \n",
    "    # store the result\n",
    "    outer_results.append(score)\n",
    "    best_inner_params_list.append(result.best_params_)\n",
    "    # report progress\n",
    "    print('>outer_test=%.3f, inner_test=%.3f, cfg=%s' % (score, result.best_score_, result.best_params_))\n",
    "\n",
    "# summarize the estimated performance of the model\n",
    "print('mean estimated performance: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0, 'max_features': 7, 'max_depth': 11.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_inner_params_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3795081081786021,\n",
       " 0.2603938897324301,\n",
       " 0.34997575039068807,\n",
       " 0.48164101073917764,\n",
       " 0.44698737542374284]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=10.0,\n",
       "                           max_features=13, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                           n_iter_no_change=100, presort='deprecated',\n",
       "                           random_state=42, subsample=0.8999999999999999,\n",
       "                           tol=0.01, validation_fraction=0.1, verbose=1,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter values for parameter (subsample) need to be a sequence(but not a string) or np.ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d76ad628b579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_inner_params_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use the fitted model to check training error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mytr_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_aleport/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1145\u001b[0m             return_train_score=return_train_score)\n\u001b[1;32m   1146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0m_check_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_aleport/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m    382\u001b[0m             if (isinstance(v, str) or\n\u001b[1;32m    383\u001b[0m                     not isinstance(v, (np.ndarray, Sequence))):\n\u001b[0;32m--> 384\u001b[0;31m                 raise ValueError(\"Parameter values for parameter ({0}) need \"\n\u001b[0m\u001b[1;32m    385\u001b[0m                                  \u001b[0;34m\"to be a sequence(but not a string) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                                  \" np.ndarray.\".format(name))\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter values for parameter (subsample) need to be a sequence(but not a string) or np.ndarray."
     ]
    }
   ],
   "source": [
    "xgb_model = GridSearchCV(best_model, param_grid=best_inner_params_list[4], scoring=scoring, n_jobs=-1, cv=cv_inner, refit=True, verbose=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted model to check training error\n",
    "ytr_p = xgb_model.predict(X_train)\n",
    "train_score = average_precision_score(y_train, ytr_p)\n",
    "\n",
    "# Use the fitted model to check test error\n",
    "yte_p = xgb_model.predict(X_test) \n",
    "test_score = average_precision_score(y_test, yte_p) #mse \n",
    "\n",
    "print('Test errors/scores of the tuned model:')\n",
    "print('mean estimated performance: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n",
    "\n",
    "print('Training and test errors/scores of the fitted model:')\n",
    "print('    score on training set (training error of tuned model) = %.8f' % train_score)\n",
    "print('    score on test set (test error of tuned model)         = %.8f' % test_score)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">outer_test=0.385, inner_test=0.487, cfg={'learning_rate': 0.1, 'n_estimators': 10}\n",
      ">outer_test=0.236, inner_test=0.507, cfg={'learning_rate': 0.1, 'n_estimators': 10}\n",
      ">outer_test=0.267, inner_test=0.487, cfg={'learning_rate': 0.1, 'n_estimators': 10}\n",
      ">outer_test=0.255, inner_test=0.534, cfg={'learning_rate': 0.1, 'n_estimators': 10}\n",
      "mean estimated performance: 0.286 (0.058)\n",
      "CPU times: user 3.57 s, sys: 54.2 ms, total: 3.62 s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X.values, y.values):\n",
    "    # split data\n",
    "    X_train, X_test = X.values[train_ix, :], X.values[test_ix, :]\n",
    "    y_train, y_test = y.values[train_ix], y.values[test_ix]\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    # define the model\n",
    "    model = GradientBoostingClassifier(random_state=1)\n",
    "    # define search space\n",
    "    parameters = {'learning_rate': [.1,.2,.3], #[0.01,0.02,0.03,0.04],\n",
    "                  #'subsample'    : [0.9], #0.5, 0.2, 0.1],\n",
    "                  'n_estimators' : [10,100,200], #,1000, 1500],\n",
    "                  #'max_depth'    : [4,6], #,8,10]\n",
    "                 }\n",
    "    # define search\n",
    "    scoring = 'average_precision' # 'neg_mean_squared_error'\n",
    "    search = GridSearchCV(model, param_grid=parameters, scoring=scoring, n_jobs=-1, cv=cv_inner, refit=True, verbose=0)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    #score = mean_squared_error(y_test, yhat)\n",
    "    score = average_precision_score(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(score)\n",
    "    # report progress\n",
    "    print('>outer_test=%.3f, inner_test=%.3f, cfg=%s' % (score, result.best_score_, result.best_params_))\n",
    "\n",
    "# summarize the estimated performance of the model\n",
    "print('mean estimated performance: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test errors/scores of the fitted model:\n",
      "    score on training set (training error of tuned model) = 0.42502365\n",
      "    score on test set (test error of tuned model)         = 0.25484604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = GridSearchCV(best_model, param_grid=result.best_params_, scoring=scoring, n_jobs=-1, cv=cv_inner, refit=True, verbose=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted model to check training error\n",
    "ytr_p = xgb_model.predict(X_train)\n",
    "train_score = average_precision_score(y_train, ytr_p)\n",
    "\n",
    "# Use the fitted model to check test error\n",
    "yte_p = xgb_model.predict(X_test) \n",
    "test_score = average_precision_score(y_test, yte_p) #mse \n",
    "\n",
    "print('Test errors/scores of the tuned model:')\n",
    "print('mean estimated performance: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n",
    "\n",
    "print('Training and test errors/scores of the fitted model:')\n",
    "print('    score on training set (training error of tuned model) = %.8f' % train_score)\n",
    "print('    score on test set (test error of tuned model)         = %.8f' % test_score)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.59126314,  5.83333588, 11.57723482,  0.58296092,  5.8234024 ,\n",
       "        11.61663119,  0.6504883 ,  5.78328196, 11.23645409]),\n",
       " 'std_fit_time': array([0.00764116, 0.00458663, 0.08935957, 0.00578645, 0.09298026,\n",
       "        0.02168486, 0.06112856, 0.02091556, 0.14044283]),\n",
       " 'mean_score_time': array([0.00538349, 0.01640201, 0.02796443, 0.00508531, 0.01494487,\n",
       "        0.02603571, 0.00583172, 0.01539771, 0.02513051]),\n",
       " 'std_score_time': array([8.71975550e-05, 5.73674671e-04, 1.82076742e-03, 1.32636798e-04,\n",
       "        3.23495113e-04, 1.51055130e-03, 9.50139309e-04, 8.19477093e-04,\n",
       "        1.46300410e-03]),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[10, 100, 200, 10, 100, 200, 10, 100, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.1, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.1, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'n_estimators': 200},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 200},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 200}],\n",
       " 'split0_test_score': array([0.52913909, 0.47635762, 0.49689564, 0.44811725, 0.42229349,\n",
       "        0.3193451 , 0.35462938, 0.4076656 , 0.41182359]),\n",
       " 'split1_test_score': array([0.51480615, 0.48968661, 0.50416599, 0.46919443, 0.5073659 ,\n",
       "        0.50701868, 0.47929859, 0.45678757, 0.45678757]),\n",
       " 'split2_test_score': array([0.55763078, 0.52733003, 0.52479562, 0.42409494, 0.39213063,\n",
       "        0.38300929, 0.33083625, 0.41882085, 0.41882085]),\n",
       " 'mean_test_score': array([0.53385867, 0.49779142, 0.50861909, 0.44713554, 0.44059667,\n",
       "        0.40312436, 0.38825474, 0.42775801, 0.429144  ]),\n",
       " 'std_test_score': array([0.01779874, 0.02158414, 0.01181735, 0.01842487, 0.0487924 ,\n",
       "        0.07792648, 0.0651064 , 0.02102612, 0.01975459]),\n",
       " 'rank_test_score': array([1, 3, 2, 4, 5, 8, 9, 7, 6], dtype=int32)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">mse=0.638, est=0.590, cfg={'learning_rate': 0.2, 'n_estimators': 200}\n",
      ">mse=0.637, est=0.648, cfg={'learning_rate': 0.2, 'n_estimators': 200}\n",
      ">mse=0.689, est=0.612, cfg={'learning_rate': 0.3, 'n_estimators': 200}\n",
      ">mse=0.653, est=0.602, cfg={'learning_rate': 0.1, 'n_estimators': 200}\n",
      "neg_mean_squared_error: 0.654 (0.021)\n",
      "CPU times: user 54.9 s, sys: 18.9 ms, total: 55 s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X.values):\n",
    "    # split data\n",
    "    X_train, X_test = X.values[train_ix, :], X.values[test_ix, :]\n",
    "    y_train, y_test = y.values[train_ix], y.values[test_ix]\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    # define the model\n",
    "    model = GradientBoostingRegressor(random_state=1)\n",
    "    # define search space\n",
    "    parameters = {'learning_rate': [.1,.2,.3], #[0.01,0.02,0.03,0.04],\n",
    "                  #'subsample'    : [0.9], #0.5, 0.2, 0.1],\n",
    "                  'n_estimators' : [10,100,200], #,1000, 1500],\n",
    "                  #'max_depth'    : [4,6], #,8,10]\n",
    "                 }\n",
    "    # define search\n",
    "    scoring = 'neg_mean_squared_error'\n",
    "    search = GridSearchCV(model, param_grid=parameters, scoring=scoring, n_jobs=-1, cv=cv_inner, refit=True, verbose=0)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    #score = mean_squared_error(y_test, yhat)\n",
    "    score = average_precision_score(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(score)\n",
    "    # report progress\n",
    "    print('>score_test=%.3f, score_est=%.3f, cfg=%s' % (score, result.best_score_, result.best_params_))\n",
    "\n",
    "# summarize the estimated performance of the model\n",
    "print('mean estimated performance: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37958,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35427,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test errors/scores of the fitted model:\n",
      "    score on training set (training error of tuned model) = 606657075.47324610\n",
      "    score on test set (test error of tuned model)         = 10548878589.98448753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = GridSearchCV(model, param_grid=parameters, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv_inner, refit=True, verbose=0)\n",
    "xgb_model.fit(Xtrain, ytrain)\n",
    "\n",
    "# Use the fitted model to check training error\n",
    "ytr_p = xgb_model.predict(Xtrain)\n",
    "train_score = mean_squared_error(ytrain, ytr_p)\n",
    "\n",
    "# Use the fitted model to check test error\n",
    "yte_p = xgb_model.predict(Xtest) \n",
    "test_score = mean_squared_error(ytest, yte_p) #mse \n",
    "print('Training and test errors/scores of the fitted model:')\n",
    "print('    score on training set (training error of tuned model) = %.8f' % train_score)\n",
    "print('    score on test set (test error of tuned model)         = %.8f' % test_score)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275978237.6719656, 279222869.1453383, 1702575340.9956436, 12235885515.391294]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(outer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1702575340.9956436,\n",
       " 1: 275978237.6719656,\n",
       " 2: 279222869.1453383,\n",
       " 3: 12235885515.391294}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(enumerate(outer_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradientBoostingRegressor' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-bec687ef651b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientBoostingRegressor' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "best_model.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_p = xgb_model.predict(data = xgb.DMatrix(data=Xtrain.sort_index().values)) \n",
    "train_score = average_precision_score(ytrain.sort_index().values, ytr_p)  #roc_auc_score\n",
    "\n",
    "# Use the fitted model to check test error\n",
    "yte_p = xgb_model.predict_proba(Xtest.sort_index().values) #yhat\n",
    "test_score = average_precision_score(ytest.sort_index().values, yte_p) #mse \n",
    "print('Training and test errors/scores of the fitted model:')\n",
    "print('    score on training set (training error of tuned model) = %.8f' % train_score)\n",
    "print('    score on test set (test error of tuned model)         = %.8f' % test_score)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.2, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=1, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.2, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=1, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.52153858,  5.26300017, 10.58611965,  0.55092994,  5.26175094,\n",
       "        10.49052215,  0.53461019,  5.2285769 , 10.07590501]),\n",
       " 'std_fit_time': array([0.00211097, 0.01611831, 0.05989154, 0.02010799, 0.03768722,\n",
       "        0.05657721, 0.01564212, 0.04863082, 0.20883988]),\n",
       " 'mean_score_time': array([0.00233229, 0.01184551, 0.02213812, 0.00228357, 0.01141667,\n",
       "        0.02137804, 0.00228087, 0.01139681, 0.01799226]),\n",
       " 'std_score_time': array([7.58081940e-05, 1.90303268e-04, 3.30895957e-04, 5.84966677e-05,\n",
       "        1.21579606e-04, 4.38652737e-04, 8.66255629e-05, 4.70253892e-04,\n",
       "        8.81935579e-04]),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[10, 100, 200, 10, 100, 200, 10, 100, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.1, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.1, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.1, 'n_estimators': 200},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.2, 'n_estimators': 200},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 10},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 100},\n",
       "  {'learning_rate': 0.3, 'n_estimators': 200}],\n",
       " 'split0_test_score': array([-2.49241276e+09, -2.52201532e+09, -2.51314155e+09, -2.57739009e+09,\n",
       "        -2.55117587e+09, -2.54168419e+09, -2.65537963e+09, -2.65482792e+09,\n",
       "        -2.65441408e+09]),\n",
       " 'split1_test_score': array([-7.92051443e+08, -9.35960190e+08, -9.61146448e+08, -9.16768850e+08,\n",
       "        -1.10184805e+09, -1.12298049e+09, -1.06186308e+09, -1.43496628e+09,\n",
       "        -1.45150537e+09]),\n",
       " 'split2_test_score': array([-5.81907817e+08, -7.71869250e+08, -7.39528299e+08, -5.66160957e+08,\n",
       "        -5.69539977e+08, -5.65851087e+08, -5.04826852e+08, -4.93054286e+08,\n",
       "        -4.94745254e+08]),\n",
       " 'mean_test_score': array([-1.28879067e+09, -1.40994825e+09, -1.40460543e+09, -1.35343997e+09,\n",
       "        -1.40752130e+09, -1.41017192e+09, -1.40735652e+09, -1.52761616e+09,\n",
       "        -1.53355490e+09]),\n",
       " 'std_test_score': array([8.55402317e+08, 7.89198467e+08, 7.89057621e+08, 8.77219815e+08,\n",
       "        8.37375784e+08, 8.31800579e+08, 9.11315387e+08, 8.84968667e+08,\n",
       "        8.83587933e+08]),\n",
       " 'rank_test_score': array([1, 6, 3, 2, 5, 7, 4, 8, 9], dtype=int32)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.02632127e+09, -4.03794576e+09, -4.11614205e+09, -4.11641480e+09])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129.53761254, 129.53761254, 129.53761254, ..., 129.53761254,\n",
       "       129.53761254, 129.53761254])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3357410081.3025613,\n",
       " 1: 493778548.8933155,\n",
       " 2: 97533439.17972018,\n",
       " 3: 167438982.33600983,\n",
       " 4: 740640387.6378227,\n",
       " 5: 745941528.0623621,\n",
       " 6: 145638275.0249669,\n",
       " 7: 298658396.5743813,\n",
       " 8: 27838745789.887886,\n",
       " 9: 614994866.0176597}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#==========================================================================\n",
    "# Keep the tuned parameters, reduce step size (eta), \n",
    "# and determine the number of trees (best_n_round).\n",
    "# Train a model on Xtr with new eta and best_n_round, and test on Xte.\n",
    "#==========================================================================\n",
    "# prepare parameters\n",
    "# parameters needed for boosters\n",
    "small_eta = 0.1\n",
    "print('reduce eta to %f and search for optimal boost rounds ......' % small_eta)\n",
    "\n",
    "# booster parameters:\n",
    "param_bst = {'booster'                : 'gbtree',     \n",
    "             'verbosity'              : 1,            \n",
    "             'objective'              : 'binary:logistic', \n",
    "             'eta'                    : small_eta,          \n",
    "             'max_depth'              : tuned_params['max_depth'],           \n",
    "             'min_child_weight'       : tuned_params['min_child_weight'],            \n",
    "             'subsample'              : tuned_params['subsample'],          \n",
    "             'colsample_bytree'       : tuned_params['colsample_bytree'],          \n",
    "             'scale_pos_weight'       : tuned_params['scale_pos_weight'],\n",
    "             'gamma'                  : tuned_params['gamma'],           \n",
    "             'lambda'                 : tuned_params['lambda'],           \n",
    "             'alpha'                  : 0,         \n",
    "             'seed'                   : 0,\n",
    "             'eval_metric'            : ['aucpr']\n",
    "             }\n",
    "\n",
    "# parameters needed for xgb.cv():\n",
    "param_cv = {'kfold'       : 5, \n",
    "            'n_round'     : 10,#5000, \n",
    "            'n_earlystop' : 10,\n",
    "            'seed_cv'     : seed_tuning, \n",
    "            'eval_metric' : ['aucpr']}\n",
    "\n",
    "# use the tuned parameters, reduce eta, use xgb.cv on Xtr, ytr to determine\n",
    "# the number of boosters\n",
    "dtrain = xgb.DMatrix(data=Xtrain.sort_index().values, label=ytrain.sort_index().values)\n",
    "\n",
    "#calibrated = CalibratedClassifierCV(xgb, method='isotonic', cv=5)\n",
    "#calibrated.fit(Xtrain, ytrain)\n",
    "\n",
    "print('start running xgb.cv ......\\n')\n",
    "cv_result = xgb.cv(params                = param_bst,  # a dictionary containing booster parameters \n",
    "                   dtrain                = dtrain, # DMatrix type\n",
    "                   num_boost_round       = param_cv['n_round'],      # max number of boosting iterations\n",
    "                   early_stopping_rounds = param_cv['n_earlystop'],  # stop if evaluation score is not improved after early_stopping_rounds of iterations\n",
    "                   nfold                 = param_cv['kfold'],        # number of folds for cv\n",
    "                   metrics               = param_cv['eval_metric'],  # 'mae', 'tweedie-nloglik@'+str(tweedie_varp), \n",
    "                   seed                  = param_cv['seed_cv'],      # seed to generate cv folds\n",
    "                   shuffle               = True,\n",
    "                   verbose_eval          = True)\n",
    "best_n_round = cv_result.shape[0] \n",
    "print('CV result:')\n",
    "print('    Best result obtained at n_round = %3d' % best_n_round)\n",
    "print('    averaged score on training folds = %.6f (std dev = %.6f)' % (cv_result.iloc[-1,0], cv_result.iloc[-1,1]))\n",
    "print('    averaged score on test folds     = %.6f (std dev = %.6f)' % (cv_result.iloc[-1,2], cv_result.iloc[-1,3]))\n",
    "print('')\n",
    "\n",
    "# train a model on Xtr, ytr  \n",
    "print('Start training a model on Xtr ......')\n",
    "\n",
    "xgb_model = xgb.train(params                = param_bst, \n",
    "                      dtrain                = dtrain, \n",
    "                      num_boost_round       = best_n_round, \n",
    "                      early_stopping_rounds = None)\n",
    "\n",
    "# calibrate the fit\n",
    "#calibrated = CalibratedClassifierCV(xgb_model, method='isotonic', cv=3)\n",
    "#calibrated.fit(dtrain)\n",
    "#ytr_p = calibrated.predict_proba(Xtrain.sort_index().values)\n",
    "#yte_p = xgb_model.predict(data = xgb.DMatrix(data=Xtest.sort_index().values))\n",
    "\n",
    "# Use the fitted model to check training error\n",
    "ytr_p = xgb_model.predict(data = xgb.DMatrix(data=Xtrain.sort_index().values)) \n",
    "train_score = average_precision_score(ytrain.sort_index().values, ytr_p)  #roc_auc_score\n",
    "\n",
    "# Use the fitted model to check test error\n",
    "yte_p = calibrated.predict_proba(Xtest.sort_index().values)\n",
    "test_score = average_precision_score(ytest.sort_index().values, yte_p) #roc_auc_score\n",
    "print('Training and test errors/scores of the fitted model:')\n",
    "print('    score on training set (training error of tuned model) = %.8f' % train_score)\n",
    "print('    score on test set (test error of tuned model)         = %.8f' % test_score)\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8 aleport",
   "language": "python",
   "name": "py38_aleport"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
